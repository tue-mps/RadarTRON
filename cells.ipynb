{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import ANCHOR_GENERATOR_REGISTRY\n",
    "\n",
    "# Print all keys in the backbone registry:\n",
    "print(list(ANCHOR_GENERATOR_REGISTRY._obj_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import BACKBONE_REGISTRY\n",
    "\n",
    "# Print all keys in the backbone registry:\n",
    "print(list(BACKBONE_REGISTRY._obj_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset and inspect a data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog\n",
    "\n",
    "# Retrieve the dataset dictionaries\n",
    "dataset_dicts = DatasetCatalog.get(\"RADIal_COCO-style\")\n",
    "\n",
    "# Print one sample to check if the \"aperture\" field is present in its annotations.\n",
    "print(\"First sample:\")\n",
    "print(dataset_dicts[0])\n",
    "\n",
    "# Optionally, loop over annotations in the sample to check each one:\n",
    "for ann in dataset_dicts[0][\"annotations\"]:\n",
    "    if \"aperture\" in ann:\n",
    "        print(\"Aperture found in annotation:\", ann[\"aperture\"])\n",
    "    else:\n",
    "        print(\"Aperture not found in this annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train loader with custom mapper and inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_train_loader\n",
    "\n",
    "train_loader = build_detection_train_loader(cfg, mapper=radar_mapper)\n",
    "data_sample = next(iter(train_loader))\n",
    "print(\"Data sample keys:\", list(data_sample[0].keys()))\n",
    "print(\"Image shape:\", data_sample[0][\"image\"].shape)\n",
    "if \"instances\" in data_sample[0]:\n",
    "    print(\"Instances fields:\", data_sample[0][\"instances\"].get_fields().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeSpec\n\u001b[0;32m----> 2\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the correct input_shape specification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m ShapeSpec(channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import ShapeSpec\n",
    "dummy_input = torch.randn(1, 32, 512, 256).to(device)\n",
    "# Create the correct input_shape specification\n",
    "input_shape = ShapeSpec(channels=32, height=512, width=256)\n",
    "\n",
    "backbone = CustomResNetBackbone(cfg, input_shape=input_shape)\n",
    "backbone = backbone.to(device)\n",
    "\n",
    "# Forward pass through the backbone\n",
    "features = backbone(dummy_input)\n",
    "\n",
    "# Print the output feature map shapes\n",
    "for key, feature_map in features.items():\n",
    "    print(f\"{key}: {feature_map.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print architecture after config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full model with your configuration\n",
    "model = build_model(cfg)\n",
    "model = model.to(device)\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dummy input with batch size 1, 32 channels, height=512, width=256\n",
    "dummy_input = torch.randn(1, 32, 512, 256)\n",
    "\n",
    "# Move the input to the correct device (GPU if available)\n",
    "dummy_input = dummy_input.to(device)\n",
    "\n",
    "# Build the full model with your configuration\n",
    "model = build_model(cfg)\n",
    "model = model.to(device)\n",
    "# Suppose dummy_input has shape [4, 32, 512, 256]\n",
    "batched_inputs = [{\"image\": dummy_input[i]} for i in range(dummy_input.shape[0])]\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(batched_inputs)\n",
    "print(\"Type of outputs:\", type(outputs))\n",
    "if isinstance(outputs, (tuple, list)):\n",
    "    print(\"Length of outputs:\", len(outputs))\n",
    "    for i, item in enumerate(outputs):\n",
    "        print(f\"Output[{i}]: type {type(item)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "output_filename = \"detections.txt\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    # Loop over each image's detections.\n",
    "    for output in outputs:\n",
    "        instances = output[\"instances\"]\n",
    "        boxes = instances.pred_boxes.tensor.cpu().numpy()  # shape (N, 4)\n",
    "        scores = instances.scores.cpu().numpy()            # shape (N,)\n",
    "        classes = instances.pred_classes.cpu().numpy()       # shape (N,)\n",
    "        # Check for your custom aperture field\n",
    "        if hasattr(instances, \"aperture\"):\n",
    "            # Ensure the aperture tensor is a 2D array of shape (N, ?)\n",
    "            aperture = instances.aperture.cpu().numpy()\n",
    "        else:\n",
    "            aperture = None\n",
    "\n",
    "        # Iterate over each detection and write a line with: class, score, boxes, aperture\n",
    "        for i in range(boxes.shape[0]):\n",
    "            cls_val = classes[i]\n",
    "            score_val = scores[i]\n",
    "            box_val = boxes[i]  # [x0, y0, x1, y1]\n",
    "            # If aperture is available, extract its value.\n",
    "            if aperture is not None:\n",
    "                # If aperture has extra dimensions, take the first element.\n",
    "                ap_val = aperture[i][0] if aperture[i].ndim > 0 else aperture[i]\n",
    "            else:\n",
    "                ap_val = \"N/A\"\n",
    "            # Create a formatted string for this detection.\n",
    "            line = f\"{cls_val}, {score_val:.4f}, {box_val.tolist()}, {ap_val}\\n\"\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check for loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.events import EventStorage\n",
    "\n",
    "# Assume cfg is already configured and updated for your custom architecture.\n",
    "# Also, device is defined (e.g., device = torch.device(\"cuda:1\"))\n",
    "\n",
    "# Create a dummy input image with 32 channels, size 512x256.\n",
    "dummy_input = torch.randn(1, 32, 512, 256, device=device)\n",
    "\n",
    "# Create dummy ground truth:\n",
    "# Let's assume you have one ground-truth object in the image.\n",
    "# For example, a box [x0, y0, x1, y1] in pixel coordinates.\n",
    "dummy_gt_boxes = Boxes(torch.tensor([[50.0, 100.0, 200.0, 120.0]], device=device))\n",
    "# Ground truth class for the object (if you have one foreground class, set it to 0).\n",
    "dummy_gt_classes = torch.tensor([0], device=device)\n",
    "# Ground truth aperture value for the object (a dummy value, e.g., 0.5).\n",
    "dummy_gt_aperture = torch.tensor([[0.5]], device=device)\n",
    "\n",
    "# Create an Instances object with the image size (height, width)\n",
    "dummy_instances = Instances(image_size=(512, 256))\n",
    "dummy_instances.gt_boxes = dummy_gt_boxes\n",
    "dummy_instances.gt_classes = dummy_gt_classes\n",
    "dummy_instances.gt_aperture = dummy_gt_aperture\n",
    "\n",
    "# Package the image and instances into a dictionary.\n",
    "batched_inputs = [{\"image\": dummy_input[0], \"instances\": dummy_instances}]\n",
    "\n",
    "# Build the model\n",
    "model = build_model(cfg)\n",
    "model = model.to(device)\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "with EventStorage():\n",
    "    losses = model(batched_inputs)\n",
    "    total_loss = sum(losses.values())\n",
    "    print(\"Total loss:\", total_loss.item())\n",
    "    total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following dataset names are not registered in the DatasetCatalog: {'RADIal_COCO-style_train'}. Available datasets are KeysView(DatasetCatalog(registered datasets: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Dataset 'RADIal_COCO-style_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/data/catalog.py:51\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/collections/__init__.py:1010\u001b[0m, in \u001b[0;36mUserDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__missing__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m-> 1010\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RADIal_COCO-style_train'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 138\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cfg\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Build the train dataloader using your custom mapper.\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_detection_train_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradar_mapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Retrieve one batch from the train loader.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/config/config.py:207\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(orig_func)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _called_with_cfg(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 207\u001b[0m         explicit_args \u001b[38;5;241m=\u001b[39m \u001b[43m_get_args_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m orig_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexplicit_args)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/config/config.py:245\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_arg_names:\n\u001b[1;32m    244\u001b[0m         extra_kwargs[name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[0;32m--> 245\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_config_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# forward the other arguments to __init__\u001b[39;00m\n\u001b[1;32m    247\u001b[0m ret\u001b[38;5;241m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/data/build.py:471\u001b[0m, in \u001b[0;36m_train_loader_from_config\u001b[0;34m(cfg, mapper, dataset, sampler)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_loader_from_config\u001b[39m(cfg, mapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 471\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_detection_dataset_dicts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASETS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilter_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATALOADER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFILTER_EMPTY_ANNOTATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_keypoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROI_KEYPOINT_HEAD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMIN_KEYPOINTS_PER_IMAGE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKEYPOINT_ON\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproposal_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASETS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPROPOSAL_FILES_TRAIN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOAD_PROPOSALS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m         _log_api_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTRAIN[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/data/build.py:253\u001b[0m, in \u001b[0;36mget_detection_dataset_dicts\u001b[0;34m(names, filter_empty, min_keypoints, proposal_files, check_consistency)\u001b[0m\n\u001b[1;32m    246\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    247\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following dataset names are not registered in the DatasetCatalog: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames_set\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable datasets are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m dataset_dicts \u001b[38;5;241m=\u001b[39m [DatasetCatalog\u001b[38;5;241m.\u001b[39mget(dataset_name) \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset_dicts[\u001b[38;5;241m0\u001b[39m], torchdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_dicts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;66;03m# ConcatDataset does not work for iterable style dataset.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m# We could support concat for iterable as well, but it's often\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# not a good idea to concat iterables anyway.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/data/build.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    247\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following dataset names are not registered in the DatasetCatalog: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames_set\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable datasets are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_datasets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m dataset_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43mDatasetCatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset_dicts[\u001b[38;5;241m0\u001b[39m], torchdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_dicts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;66;03m# ConcatDataset does not work for iterable style dataset.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m# We could support concat for iterable as well, but it's often\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# not a good idea to concat iterables anyway.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/radarTron/lib/python3.8/site-packages/detectron2/data/catalog.py:53\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not registered! Available datasets are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     55\u001b[0m             name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f()\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Dataset 'RADIal_COCO-style_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.events import EventStorage\n",
    "import numpy as np\n",
    "import cv2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2 import model_zoo\n",
    "from custom_anchorGen2 import CustomAnchorGenerator\n",
    "from custom_backbone import CustomResNetBackbone\n",
    "from CustomFastRCNNOutputLayers import CustomFastRCNNOutputLayers\n",
    "from CustomStandardROIHeads import CustomStandardROIHeads\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "# Load Detectron2 default model config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")) \n",
    "# Set device in config\n",
    "cfg.MODEL.WEIGHTS = \"\"\n",
    "cfg.MODEL.DEVICE = \"cuda:1\"  # Add this line\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.DATASETS.TRAIN = (\"RADIal_COCO-style_train\",)\n",
    "cfg.DATASETS.TEST = (\"RADIal_COCO-style_val\",)\n",
    "\n",
    "# Manually set the required fields for a ResNet-FPN backbone:\n",
    "cfg.MODEL.RESNETS.DEPTH = 50  # or 101, depending on your design\n",
    "cfg.MODEL.RESNETS.STEM_OUT_CHANNELS = 192\n",
    "cfg.MODEL.RESNETS.OUT_FEATURES = [\"res2\", \"res3\", \"res4\"] \n",
    "cfg.MODEL.RESNETS.NORM = \"\"\n",
    "cfg.MODEL.FPN.IN_FEATURES = [\"res2\", \"res3\", \"res4\"]\n",
    "\n",
    "# Update anchor generator config\n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.STRIDES = [1, 2, 4]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[71], [71], [71]]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.078125]]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.OFFSET = 0.5\n",
    "cfg.MODEL.ANCHOR_GENERATOR.STD_BEHAVIOR = True\n",
    "\n",
    "# Use your custom ROI heads that instantiate the custom box predictor\n",
    "cfg.MODEL.ROI_HEADS.NAME = \"CustomStandardROIHeads\"\n",
    "\n",
    "# Update box regression weights to include the extra aperture output:\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (1.0, 5.0, 1.0, 5.0)\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE = \"smooth_l1\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT = 1.0\n",
    "cfg.MODEL.ROI_BOX_HEAD.APERTURE_LOSS_WEIGHT = 300.0\n",
    "cfg.MODEL.ROI_BOX_HEAD.LOSS_CLS_WEIGHT=1.4\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.IN_FEATURES = [\"p2\", \"p3\", \"p4\"]\n",
    "cfg.MODEL.RPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\"]\n",
    "cfg.MODEL.FPN.OUT_CHANNELS = 256\n",
    "cfg.MODEL.FPN.NORM = \"\"\n",
    "cfg.MODEL.FPN.FUSE_TYPE = \"sum\"\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 0\n",
    "# Set the backbone\n",
    "cfg.MODEL.BACKBONE.NAME = 'CustomResNetBackbone'\n",
    "cfg.MODEL.PIXEL_MEAN = [-2.6244e-03, -2.1335e-01,  1.8789e-02, -1.4427e+00, -3.7618e-01,\n",
    "                1.3594e+00, -2.2987e-01,  1.2244e-01,  1.7359e+00, -6.5345e-01,\n",
    "                3.7976e-01,  5.5521e+00,  7.7462e-01, -1.5589e+00, -7.2473e-01,\n",
    "                1.5182e+00, -3.7189e-01, -8.8332e-02, -1.6194e-01,  1.0984e+00,\n",
    "                9.9929e-01, -1.0495e+00,  1.9972e+00,  9.2869e-01,  1.8991e+00,\n",
    "               -2.3772e-01,  2.0000e+00,  7.7737e-01,  1.3239e+00,  1.1817e+00,\n",
    "               -6.9696e-01,  4.4288e-01] \n",
    "cfg.MODEL.PIXEL_STD = [20775.3809, 23085.5000, 23017.6387, 14548.6357, 32133.5547, 28838.8047,\n",
    "                27195.8945, 33103.7148, 32181.5273, 35022.1797, 31259.1895, 36684.6133,\n",
    "                33552.9258, 25958.7539, 29532.6230, 32646.8984, 20728.3320, 23160.8828,\n",
    "                23069.0449, 14915.9053, 32149.6172, 28958.5840, 27210.8652, 33005.6602,\n",
    "                31905.9336, 35124.9180, 31258.4316, 31086.0273, 33628.5352, 25950.2363,\n",
    "                29445.2598, 32885.7422]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = 'CustomAnchorGenerator'\n",
    "\n",
    "import torch\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "\n",
    "# Build the train dataloader using your custom mapper.\n",
    "train_loader = build_detection_train_loader(cfg, mapper=radar_mapper)\n",
    "\n",
    "# Retrieve one batch from the train loader.\n",
    "data_batch = next(iter(train_loader))\n",
    "print(\"A batch from the train loader:\")\n",
    "print(\"Keys in batch sample:\", data_batch[0].keys())\n",
    "print(\"Image shape:\", data_batch[0][\"image\"].shape)\n",
    "if \"instances\" in data_batch[0]:\n",
    "    print(\"Instances fields:\", data_batch[0][\"instances\"].get_fields().keys())\n",
    "\n",
    "# Build the model.\n",
    "model = build_model(cfg)\n",
    "model = model.to(\"cuda:1\")  # or whichever device you are using\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "# Wrap the forward pass within an EventStorage context.\n",
    "with EventStorage():\n",
    "    losses = model(data_batch)\n",
    "    total_loss = sum(losses.values())\n",
    "    print(\"Losses:\", {k: v.item() for k, v in losses.items()})\n",
    "    print(\"Total loss:\", total_loss.item())\n",
    "    total_loss.backward()\n",
    "    print(\"Backward pass successful!\")\n",
    "# --- Now, inspect gradients for the aperture branch ---\n",
    "# Access the box predictor (assumed to be under roi_heads.box_predictor)\n",
    "predictor = model.roi_heads.box_predictor\n",
    "\n",
    "# Get the gradient of the final linear layer that produces regression outputs.\n",
    "# Its shape is [num_bbox_reg_outputs, in_features]\n",
    "weight_grad = predictor.bbox_pred.weight.grad  # shape: [num_bbox_reg_outputs, in_features]\n",
    "\n",
    "# In your custom predictor, total_reg_dim = 5 (first 4 for box deltas, 5th for aperture)\n",
    "total_reg_dim = predictor.total_reg_dim  # should be 5\n",
    "# Calculate the number of regression heads (e.g., 1 if class-agnostic)\n",
    "num_bbox_reg_classes = weight_grad.shape[0] // total_reg_dim\n",
    "\n",
    "# Reshape the weight gradient so that the aperture gradients are in a separate slice.\n",
    "# New shape: [num_bbox_reg_classes, total_reg_dim, in_features]\n",
    "reshaped_weight_grad = weight_grad.view(num_bbox_reg_classes, total_reg_dim, -1)\n",
    "\n",
    "# Extract the gradients for the aperture output (index 4 in each group)\n",
    "aperture_weight_grad = reshaped_weight_grad[:, 4, :]  # shape: [num_bbox_reg_classes, in_features]\n",
    "\n",
    "# Compute the norm (L2) of the gradients for each regression head's aperture part.\n",
    "aperture_grad_norms = aperture_weight_grad.norm(dim=1)\n",
    "print(\"Aperture weight gradient norms per regression head:\", aperture_grad_norms)\n",
    "\n",
    "# Similarly, check the bias gradients for the aperture.\n",
    "bias_grad = predictor.bbox_pred.bias.grad.view(num_bbox_reg_classes, total_reg_dim)\n",
    "aperture_bias_grad = bias_grad[:, 4]\n",
    "print(\"Aperture bias gradient norms per regression head:\", aperture_bias_grad.abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor generator inspector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = model.proposal_generator.anchor_generator\n",
    "\n",
    "# Debug configuration values\n",
    "print(\"\\nConfiguration Details:\")\n",
    "print(f\"Sizes configuration: {cfg.MODEL.ANCHOR_GENERATOR.SIZES}\")\n",
    "print(f\"Aspect ratios configuration: {cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS}\")\n",
    "print(f\"Strides configuration: {cfg.MODEL.ANCHOR_GENERATOR.STRIDES}\")\n",
    "\n",
    "# Debug anchor generator state\n",
    "print(\"\\nAnchor Generator State:\")\n",
    "print(f\"Strides: {anchor_generator.strides}\")\n",
    "print(f\"Number of features: {anchor_generator.num_features}\")\n",
    "print(f\"Number of cell anchors: {anchor_generator.num_cell_anchors}\")\n",
    "\n",
    "# Detailed cell anchor inspection\n",
    "print(\"\\nDetailed Cell Anchors:\")\n",
    "for i, cell_anchor in enumerate(anchor_generator.cell_anchors):\n",
    "    print(f\"\\nLevel {i}:\")\n",
    "    print(f\"Shape: {cell_anchor.shape}\")\n",
    "    if len(cell_anchor) > 0:\n",
    "        print(f\"All anchor boxes at this level:\")\n",
    "        print(cell_anchor)\n",
    "        \n",
    "        # Calculate actual dimensions\n",
    "        if not torch.all(cell_anchor == 0):\n",
    "            widths = cell_anchor[:, 2] - cell_anchor[:, 0]\n",
    "            heights = cell_anchor[:, 3] - cell_anchor[:, 1]\n",
    "            print(f\"Anchor widths: {widths}\")\n",
    "            print(f\"Anchor heights: {heights}\")\n",
    "            print(f\"Aspect ratios: {heights/widths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature maps from your backbone (we need this to see actual coverage)\n",
    "features = backbone(dummy_input)\n",
    "\n",
    "# Get anchors for these feature maps\n",
    "anchors = anchor_generator(list(features.values()))\n",
    "\n",
    "print(\"Effective Coverage Analysis:\")\n",
    "for level_idx, (level_name, feature_map) in enumerate(features.items()):\n",
    "    # Get feature map dimensions\n",
    "    fmap_h, fmap_w = feature_map.shape[-2:]\n",
    "    \n",
    "    # Get stride for this level\n",
    "    stride = anchor_generator.strides[level_idx]\n",
    "    \n",
    "    # Get anchors for this level\n",
    "    level_anchors = anchors[level_idx].tensor\n",
    "    \n",
    "    # Get first anchor dimensions in absolute coordinates\n",
    "    first_anchor = level_anchors[0]\n",
    "    abs_width = first_anchor[2] - first_anchor[0]\n",
    "    abs_height = first_anchor[3] - first_anchor[1]\n",
    "    \n",
    "    # Calculate effective coverage (in feature map units)\n",
    "    eff_width = abs_width / stride\n",
    "    eff_height = abs_height / stride\n",
    "    \n",
    "    print(f\"\\nLevel {level_name}:\")\n",
    "    print(f\"Feature map size: {fmap_h}×{fmap_w}\")\n",
    "    print(f\"Stride: {stride}\")\n",
    "    print(f\"Absolute anchor size: {abs_width:.2f}×{abs_height:.2f}\")\n",
    "    print(f\"Effective coverage (feature map units): {eff_width:.2f}×{eff_height:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_coverage(feature_map_size, effective_coverage, level_name):\n",
    "    fig, ax = plt.subplots(figsize=(6, 9))\n",
    "    # Plot feature map bounds\n",
    "    rect_map = plt.Rectangle((0, 0), feature_map_size[1], feature_map_size[0], \n",
    "                           fill=False, color='blue', label='Feature Map')\n",
    "    # Plot anchor coverage\n",
    "    rect_anchor = plt.Rectangle(\n",
    "        ((feature_map_size[1] - effective_coverage[0])/2, \n",
    "         (feature_map_size[0] - effective_coverage[1])/2),\n",
    "        effective_coverage[0], effective_coverage[1],\n",
    "        fill=False, color='red', label='Anchor Coverage')\n",
    "    \n",
    "    ax.add_patch(rect_map)\n",
    "    ax.add_patch(rect_anchor)\n",
    "    ax.set_xlim(-10, feature_map_size[1] + 10)\n",
    "    ax.set_ylim(-10, feature_map_size[0] + 10)\n",
    "    ax.set_title(f'Level {level_name} Coverage')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each level\n",
    "for level_name, feature_map in features.items():\n",
    "    fmap_h, fmap_w = feature_map.shape[-2:]\n",
    "    level_idx = int(level_name[1]) - 2  # p2->0, p3->1, p4->2\n",
    "    \n",
    "    effective_width = 254.02 / anchor_generator.strides[level_idx]\n",
    "    effective_height = 19.85 / anchor_generator.strides[level_idx]\n",
    "    \n",
    "    plot_coverage((fmap_h, fmap_w), \n",
    "                 (effective_width, effective_height),\n",
    "                 level_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get template anchors (cell anchors)\n",
    "print(\"Template Anchors (Cell Anchors):\")\n",
    "for i, cell_anchor in enumerate(anchor_generator.cell_anchors):\n",
    "    print(f\"\\nLevel {i}:\")\n",
    "    print(f\"Template shape: {cell_anchor.shape}\")\n",
    "    print(f\"Template anchor: {cell_anchor[0]}\")\n",
    "\n",
    "# Get actual generated anchors\n",
    "features_list = [features[k] for k in [\"p2\", \"p3\", \"p4\"]]\n",
    "anchors = anchor_generator(features_list)\n",
    "\n",
    "print(\"\\nGenerated Anchors Position Analysis:\")\n",
    "total_anchors = 0\n",
    "\n",
    "for i, (level_name, feature_map) in enumerate(features.items()):\n",
    "    fmap_h, fmap_w = feature_map.shape[-2:]\n",
    "    expected_center = (fmap_w * anchor_generator.strides[i]) / 2\n",
    "    \n",
    "    # Get all anchors for this level\n",
    "    level_anchors = anchors[i].tensor\n",
    "    num_anchors = len(level_anchors)\n",
    "    total_anchors += num_anchors\n",
    "    \n",
    "    # Calculate centers of anchors\n",
    "    anchor_centers_x = (level_anchors[:, 0] + level_anchors[:, 2]) / 2\n",
    "    \n",
    "    # Calculate expected number of anchors\n",
    "    expected_anchors = fmap_h * fmap_w if anchor_generator.std_behavior else fmap_h\n",
    "    \n",
    "    print(f\"\\nLevel {level_name}:\")\n",
    "    print(f\"Feature map size: {fmap_h}×{fmap_w}\")\n",
    "    print(f\"Number of anchors: {num_anchors}\")\n",
    "    print(f\"Expected number of anchors: {expected_anchors}\")\n",
    "    print(f\"Behavior matches config?: {num_anchors == expected_anchors}\")\n",
    "    print(f\"Expected center position: {expected_center}\")\n",
    "    print(f\"Actual anchor centers (first 5): {anchor_centers_x[:5]}\")\n",
    "    print(f\"All centers same?: {torch.allclose(anchor_centers_x, anchor_centers_x[0])}\")\n",
    "    print(f\"Center matches expected?: {torch.allclose(anchor_centers_x[0], torch.tensor(expected_center, device='cuda:1'))}\")\n",
    "\n",
    "print(f\"\\nTotal number of anchors across all levels: {total_anchors}\")\n",
    "print(f\"Using {'standard' if anchor_generator.std_behavior else 'center-only'} behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor head output inspector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy forward pass example\n",
    "\n",
    "import torch\n",
    "from detectron2.layers import ShapeSpec\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from CustomFastRCNNOutputLayers import CustomFastRCNNOutputLayers\n",
    "\n",
    "# Create a dummy ShapeSpec matching your configuration\n",
    "input_shape = ShapeSpec(channels=256, height=7, width=7)\n",
    "\n",
    "# Set up a dummy box2box transform with 4 weights (ignore aperture here)\n",
    "dummy_box2box_transform = Box2BoxTransform(weights=(1.0, 10.0, 1.0, 5.0))\n",
    "\n",
    "# Instantiate your custom predictor using dummy parameters\n",
    "num_classes = 1   # as in your config\n",
    "predictor = CustomFastRCNNOutputLayers(\n",
    "    input_shape,\n",
    "    box2box_transform=dummy_box2box_transform,\n",
    "    num_classes=num_classes,\n",
    "    cls_agnostic_bbox_reg=True,   # according to your config\n",
    "    smooth_l1_beta=0.0,\n",
    "    box_reg_loss_type=\"smooth_l1\",\n",
    "    test_score_thresh=0.0,\n",
    "    test_nms_thresh=0.5,\n",
    "    test_topk_per_image=100,\n",
    "    aperture_loss_weight=2.0,  # as set in your config\n",
    ")\n",
    "\n",
    "# Set to evaluation mode\n",
    "predictor.eval()\n",
    "\n",
    "# Create a dummy input tensor that matches the expected shape (e.g., a batch of 2 regions)\n",
    "dummy_input = torch.randn(16, 256, 7, 7)\n",
    "\n",
    "# Forward pass through the predictor.\n",
    "with torch.no_grad():\n",
    "    scores, proposal_deltas = predictor(dummy_input)\n",
    "\n",
    "print(\"Scores shape:\", scores.shape)\n",
    "print(\"Proposal deltas shape:\", proposal_deltas.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radarTron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
